<!DOCTYPE html>
<html lang="" xml:lang="">
  <head>
    <title> PPOL561 | Accelerated Statistics for Public Policy II  Week 13       Ordered and Multinomial Outcomes</title>
    <meta charset="utf-8" />
    <meta name="author" content="  Prof. Eric Dunford  ◆  Georgetown University  ◆  McCourt School of Public Policy  ◆  eric.dunford@georgetown.edu" />
    <link rel="stylesheet" href="gu-theme.css" type="text/css" />
  </head>
  <body>
    <textarea id="source">
class: center, middle, inverse, title-slide

# <font class = "title-panel"> PPOL561 | Accelerated Statistics for Public Policy II</font> <font size=6, face="bold"> Week 13 </font> <br> <br> <font size=100, face="bold"> Ordered and Multinomial Outcomes </font>
### <font class = "title-footer">  Prof. Eric Dunford  ◆  Georgetown University  ◆  McCourt School of Public Policy  ◆  <a href="mailto:eric.dunford@georgetown.edu" class="email">eric.dunford@georgetown.edu</a></font>

---




layout: true

&lt;div class="slide-footer"&gt;&lt;span&gt; 
PPOL561 | Accelerated Statistics for Public Policy II

&amp;emsp;&amp;emsp;&amp;emsp;&amp;emsp;&amp;emsp;&amp;emsp;&amp;emsp;&amp;emsp;&amp;emsp;

Week 13 &lt;!-- Week of the Footer Here --&gt;

&amp;emsp;&amp;emsp;&amp;emsp;&amp;emsp;&amp;emsp;&amp;emsp;&amp;emsp;&amp;emsp;&amp;emsp;&amp;emsp;&amp;emsp;&amp;emsp;

Ordered and Multinomial Outcomes &lt;!-- Title of the lecture here --&gt;

&lt;/span&gt;&lt;/div&gt; 

---
class: outline

# Outline for Today 

&lt;br&gt;&lt;br&gt;
- Modeling **ordered outcomes**

&lt;br&gt;&lt;br&gt;
- Modeling **nominal outcomes**


---

class: newsection

# Ordered Outcomes

---

### Ordered outcomes

- Ordered dependent variables are outcomes that retain some inherent but discrete ordering. 

  - **Likert Scales**: "Strongly Disagree", "Disagree", "Somewhat Disagree", "Somewhat Agree", ...
  - **Policy options**: "privatize social security", "paritally privatize", "leave unchanged"
  - **Ranks**: "some high School", "high school grad", "some college", "college grad", etc.

- The distance between each category is of different unknown sizes. 

- When the proper ordering of a variable is ambiguous, multinomial models should be considered.

---

### Why not OLS?

&lt;br&gt;

- Encounter the same problems as when using regression with a binary outcome:
  + _heteroskedastic_
  + _non-sensical predictions_
  
- Requires us to assume that the **distance between categories is equal**.

- Difficult to interpret

---

### Latent Variable Approach 
&lt;br&gt;&lt;br&gt;
Recall when discussing binary models we spoke about things in terms of a **latent variable** `\(y^*\)`. 

`$$y_i^* = \beta_0 + \beta_1x_i + \epsilon_i$$`

When `\(y^*\)` is above a threshold `\(\tau_1\)` we observe `\(y = 1\)`, otherwise we observe `\(y = 0\)`. 

In this model, we assumed that the cutoff was 0 ( `\(\tau_1 = 0\)` ). 

---

&lt;img src="week-13_ordered-multinomial_ppol561_files/figure-html/unnamed-chunk-1-1.png" style="display: block; margin: auto;" /&gt;


---

&lt;img src="week-13_ordered-multinomial_ppol561_files/figure-html/unnamed-chunk-2-1.png" style="display: block; margin: auto;" /&gt;

---


&lt;img src="week-13_ordered-multinomial_ppol561_files/figure-html/unnamed-chunk-3-1.png" style="display: block; margin: auto;" /&gt;


---

The probability of observing a specific response in the ordering _depends on which cut points
`\(y^∗\)` falls in-between_.

- when &lt;font color = "grey"&gt; `\(y^* \le \tau_1\)` &lt;/font&gt;, we observe &lt;font color = "grey"&gt; `\(y=1\)` &lt;/font&gt;
- when &lt;font color = "steelblue"&gt; `\(\tau_1 \le y^* &lt; \tau_2\)` &lt;/font&gt;, we observe &lt;font color = "steelblue"&gt; `\(y=2\)` &lt;/font&gt;
- when &lt;font color = "orange"&gt; `\(\tau_2 \ge y^*\)` &lt;/font&gt;, we observe &lt;font color = "orange"&gt; `\(y=3\)` &lt;/font&gt;

&lt;img src="week-13_ordered-multinomial_ppol561_files/figure-html/unnamed-chunk-4-1.png" style="display: block; margin: auto;" /&gt;


---

### Identification
&lt;br&gt;&lt;br&gt;
- To identify the model, we need to do one of two things

  - set the **intercept** ( `\(\beta_0\)` ) **equal to 0**
  
  - set the ** `\(\tau_0\)` ** (our `\(-\infty\)` cut point) ** equal to 0**
  
- The `polr()` function from the `MASS` package (which we'll use to estimate a ordered probit and logit model) opts to set the intercept to 0 by default

---

### Parallel Regressions Assumption

&lt;br&gt;&lt;br&gt;

- When running the an ordered logit or probit model, we inherently make an assumption about "**parallel regressions**".

- The effect of the coefficients is the same across all cutpoint ranges, i.e. the effect of `\(x\)` on `\(y\)` does not change when we move from one threshold to the next.

- Because the relationship between all pairs of groups is the same, there is **_only one set of coefficients_** (only one model).  If this was not the case, we would need different models to describe the relationship between each pair of outcome groups. 

---

&lt;img src="week-13_ordered-multinomial_ppol561_files/figure-html/unnamed-chunk-5-1.png" style="display: block; margin: auto;" /&gt;

---

&lt;img src="week-13_ordered-multinomial_ppol561_files/figure-html/unnamed-chunk-6-1.png" style="display: block; margin: auto;" /&gt;


---

### Predicted Probabilities

The key to calculating the predicted probabilities of any ordered outcome is **_keeping track of where the cutpoints fall_**. 

&lt;br&gt;

.center[
|               |                                |
| --------------|--------------------------------|
| `\(pr(y_i = 1)\)` | `\(\Phi(\tau_1 - \beta_1x_i) - 0\)` |
| `\(pr(y_i = 2)\)` | `\(\Phi(\tau_2 - \beta_1 x_i) - \Phi(\tau_1 - \beta_1 x_i)\)` |
| `\(pr(y_i = 3)\)` | `\(1 - \Phi(\tau_2 - \beta_1 x_i)\)` | 
]


&lt;br&gt;

Recall that this is a _probability distribution_, where the bounds fall between 0 and 1. Thus, why the lower bound is 0 and the upper bound is 1. 

---

### Predicted Probabilities

For example, say `\(\tau_1 = -1\)`, `\(\tau_2 = 1\)`, `\(\beta_1 =.05\)`. 

We can calculate the predicted probabilities of `\(y\)` equalling a 1, 2 or 3 when `\(x = 15\)` as follows

&lt;br&gt;

`$$pr(y_i = 1) = \Phi((-1) - (.05)(15)) = .04$$`

`$$pr(y_i = 2) = \Phi((1) - (.05)(15)) - \Phi((-1) - (.05)(15)) = .56$$`

`$$pr(y_i = 3) = 1 - \Phi((1) - (.05)(15)) = .40$$`

&lt;br&gt;

In this example, for an observation where `\(x = 15\)`, the probability that observed response will be 1 is 4%, 2 is 56%, and 3 is 40%. 

---

### Example: Student Support for the Iraq War

Survey data draw from a sample of 500 college students in 2002 leading up to the Iraq war. The survey asked whether college students agreed with the United States entering into the Iraq War. 

--

Three variables: 
  + `warsup`:  whether the respondent somewhat or strongly oppose/supported the War in Iraq
  
  + `dem`: 1 if the repondent was a democrat, 0 otherwise.
  
  + `female`: 1 if the repondent is female, 0 otherwise.

--

&lt;br&gt; 
Our hypothesis is that democratic respondents are less likely to support the Iraq War than non-democratic respondents.


---



&lt;br&gt;&lt;br&gt;&lt;br&gt;&lt;br&gt;

```r
summary(dat)
```

```
##               warsup         dem            female     
##  strongly oppose :168   Min.   :0.000   Min.   :0.000  
##  somewhat oppose : 98   1st Qu.:0.000   1st Qu.:0.000  
##  somewhat support:126   Median :1.000   Median :1.000  
##  strongly support:108   Mean   :0.536   Mean   :0.576  
##                         3rd Qu.:1.000   3rd Qu.:1.000  
##                         Max.   :1.000   Max.   :1.000
```

---


```r
war_ordered &lt;- MASS::polr(warsup ~ dem + female, # Model
                          method = "probit", # Probit model
                          data = dat)
summary(war_ordered,digits = 2)
```

```
## Call:
## MASS::polr(formula = warsup ~ dem + female, data = dat, method = "probit")
## 
## Coefficients:
##        Value Std. Error t value
## dem    -1.50       0.11   -13.7
## female -0.18       0.10    -1.7
## 
## Intercepts:
##                                   Value  Std. Error t value
## strongly oppose|somewhat oppose    -1.49   0.11     -13.52 
## somewhat oppose|somewhat support   -0.83   0.10      -8.16 
## somewhat support|strongly support   0.13   0.10       1.34 
## 
## Residual Deviance: 1159.039 
## AIC: 1169.039
```

---

What is the probability of "Somewhat Supporting" the war given one is a democrat?

--

&lt;br&gt;


```r
# Grab the coefficients
B = war_ordered$coefficients
B
```

```
##        dem     female 
## -1.4998994 -0.1786577
```

&lt;br&gt;


```r
# Grab the cut points
cuts = war_ordered$zeta
cuts
```

```
##   strongly oppose|somewhat oppose  somewhat oppose|somewhat support 
##                        -1.4893715                        -0.8277817 
## somewhat support|strongly support 
##                         0.1277792
```

---

What is the probability of "Somewhat Supporting" the war given one is a democrat?

&lt;br&gt;


```r
# Grab the data
X = model.matrix(war_ordered)[,-1] # drop intercept
head(X)
```

```
##   dem female
## 1   0      0
## 2   0      1
## 3   0      1
## 4   1      1
## 5   0      0
## 6   0      0
```


---

What is the probability of "Somewhat Supporting" the war given one is a democrat?

&lt;br&gt;

```r
# Manipulate one value
X[,1] = 1 # Respondent is a democrat
```

&lt;br&gt;
Calculate the predicted probability for the "Somwhat support" category.


```r
pr_some_supp = pnorm(cuts[3] - X%*%B) - pnorm(cuts[2] - X%*%B)
mean(pr_some_supp)
```

```
## [1] 0.1776783
```

---

What is the probability of "Somewhat Supporting" the war given one is _not_ a democrat?

&lt;br&gt;

```r
# Manipulate one value
*X[,1] = 0 # Respondent is NOT a democrat
```

&lt;br&gt;
Calculate the predicted probability for the "Somwhat support" category.


```r
pr_some_supp = pnorm(cuts[3] - X%*%B) - pnorm(cuts[2] - X%*%B)
mean(pr_some_supp)
```

```
## [1] 0.3557503
```

---

&lt;br&gt;&lt;br&gt;
The discrete difference between "Somewhat Supporting" the war given one...

&lt;br&gt;

- ...is a democrat = **17.7%**

- ...is _not_ a democrat = **35.6%**

&lt;br&gt;&lt;br&gt;
The probability of "Somewhat Supporting" the war **decreases by 17.8%** if the respondent is a Democrat verse if he or she is not.

---

### Simulating Confidence Intervals


```r
require(obsval) # Load the package

# Re-estimate the model using obsval
war_ordered2 &lt;- 
  obsval(warsup ~ dem + female,
         data = dat,
         ci = .95,
         n.draws = 1000,
         reg.model = "oprobit",
         effect.var = "dem",
         effect.vals = c(0,1))
```


```r
# Effect summary
round(war_ordered2$effect_sum,3)
```

```
##                    2.5%   Mean  97.5%
## strongly oppose   0.396  0.460  0.519
## somewhat oppose   0.052  0.083  0.118
## somewhat support -0.224 -0.176 -0.130
## strongly support -0.425 -0.366 -0.310
```

---

&lt;br&gt;

```r
war_ordered2$high.ci
```

```
##       strongly oppose somewhat oppose somewhat support strongly support
## var_0       0.1160783       0.1861273        0.4031105       0.47020427
## var_1       0.6019161       0.2749862        0.2168009       0.06418976
```


```r
war_ordered2$means
```

```
##       strongly oppose somewhat oppose somewhat support strongly support
## var_0      0.08463486       0.1520751        0.3535692       0.40972083
## var_1      0.54433086       0.2351130        0.1771078       0.04344832
```


```r
war_ordered2$low.ci
```

```
##       strongly oppose somewhat oppose somewhat support strongly support
## var_0      0.05685161       0.1212347        0.2993841        0.3507988
## var_1      0.48175780       0.1957167        0.1418010        0.0272275
```


---

&lt;img src="week-13_ordered-multinomial_ppol561_files/figure-html/unnamed-chunk-22-1.png" style="display: block; margin: auto;" /&gt;

---

class:newsection

# Multinomial Outcomes

---

### Multinomial Outcomes

- A nomial outcome variable is a variable with **discrete categories but no intrinsic ordering** 

- See this when a variable is just "names" or stated preferences. 

- The focus is analyzing **questions of "what", "which", "who", and "where"**, rather than questions regarding "how much" or "how many".

- Examples: 
  + Who did you vote for? 
  + Which product did you purchase?
  + What kind of job do you have?
  + What is the ethnicity of a new hire?
  
---

Say we have **three candidates** in an election: 

&lt;img src="week-13_ordered-multinomial_ppol561_files/figure-html/unnamed-chunk-23-1.png" style="display: block; margin: auto;" /&gt;


---

Say we have **three candidates** in an election: 

&lt;img src="week-13_ordered-multinomial_ppol561_files/figure-html/unnamed-chunk-24-1.png" style="display: block; margin: auto;" /&gt;

A voter is deciding which candidate to vote for. 

How do we figure out the probability of voting for one candidate (say, Sally) given one can also vote for Bruce or Jack?

---

One solution would be to treat the problem as a binary response.

&lt;img src="week-13_ordered-multinomial_ppol561_files/figure-html/unnamed-chunk-25-1.png" style="display: block; margin: auto;" /&gt;

We just calculated the probability of voting for Sally (1) or not (0). 


**$$pr(vote_{sally} = 1) = \frac{exp(x_i\beta^{sally})}{1 + exp(x_i\beta^{sally})}$$**

---

One solution would be to treat the problem as a binary response.

&lt;img src="week-13_ordered-multinomial_ppol561_files/figure-html/unnamed-chunk-26-1.png" style="display: block; margin: auto;" /&gt;

We can just re-arrange to calculate the probability another candidate. 
**$$pr(vote_{bruce} = 1) = \frac{exp(x_i\beta^{bruce})}{1 + exp(x_i\beta^{bruce})}$$**

---

One solution would be to treat the problem as a binary response.

&lt;img src="week-13_ordered-multinomial_ppol561_files/figure-html/unnamed-chunk-27-1.png" style="display: block; margin: auto;" /&gt;

We can just re-arrange to calculate the probability another candidate. 
**$$pr(vote_{jack} = 1) = \frac{exp(x_i\beta^{jack})}{1 + exp(x_i\beta^{jack})}$$**

---

### Multinomial model

The multinomial logit model allows us to estimate all these binary logits for the different outcome categories simultaneously.

&lt;br&gt;

We can estimate the probability of making one choice _given all the alternative options_.

&lt;br&gt;

`$$pr(vote = sally) = \frac{exp(x_i\beta^{sally})}{exp(x_i\beta^{bruce}) + exp(x_i\beta^{jack}) + exp(x_i\beta^{sally})}$$`

---

### Identification

To identify the model, we need to constrain the coefficients for one of the choices (i.e. candidates) to equal 0. This choice will be the **reference category**.

From our candidates, let's choose Bruce to be the reference category: `$$\beta^{bruce}=0$$` 

Thus, the multinomial logit model is as follows:

&lt;br&gt;

`$$pr(vote = sally) = \frac{exp(x_i\beta^{sally})}{1 + exp(x_i\beta^{jack}) + exp(x_i\beta^{sally})}$$`

---

### Predicted Probability

To figure out probability of voting for any of the other candidates, we just need to re-arrange this equation.

&lt;br&gt;

`$$pr(vote = jack) = \frac{exp(x_i\beta^{jack})}{1 + exp(x_i\beta^{jack}) + exp(x_i\beta^{sally})}$$`

&lt;br&gt;&lt;br&gt;

`$$pr(vote = bruce) = \frac{1}{1 + exp(x_i\beta^{jack}) + exp(x_i\beta^{sally})}$$`

---

### Predicted Probability

Say `\(x_i\)` is a dummy variable that takes on the value of 0 and 1. To calculate the probability of voting for **Sally** given `\(x_i=1\)` we do as follow:

`$$\frac{exp(\beta_0^{sally} + \beta_1^{sally} x_i)}{1 + exp(\beta_0^{jack} + \beta_1^{jack} x_i) + exp(\beta_0^{sally} + \beta_1^{sally} x_i)}$$`

&lt;br&gt;

** `$$\frac{exp(\beta_0^{sally} + \beta_1^{sally} (1))}{1 + exp(\beta_0^{jack} + \beta_1^{jack} (1)) + exp(\beta_0^{sally} + \beta_1^{sally} (1))}$$` **

---

### Predicted Probability

Say `\(x_i\)` is a dummy variable that takes on the value of 0 and 1. To calculate the probability of voting for **Jack** given `\(x_i=1\)` we do as follow:

`$$\frac{exp(\beta_0^{jack} + \beta_1^{jack} x_i)}{1 + exp(\beta_0^{jack} + \beta_1^{jack} x_i) + exp(\beta_0^{sally} + \beta_1^{sally} x_i)}$$`

&lt;br&gt;

** `$$\frac{exp(\beta_0^{jack} + \beta_1^{jack} (1))}{1 + exp(\beta_0^{jack} + \beta_1^{jack} (1)) + exp(\beta_0^{sally} + \beta_1^{sally} (1))}$$` **

---

### Predicted Probability

Say `\(x_i\)` is a dummy variable that takes on the value of 0 and 1. To calculate the probability of voting for **Bruce** given `\(x_i=1\)` we do as follow:

`$$\frac{1}{1 + exp(\beta_0^{jack} + \beta_1^{jack} x_i) + exp(\beta_0^{sally} + \beta_1^{sally} x_i)}$$`

&lt;br&gt;

** `$$\frac{1}{1 + exp(\beta_0^{jack} + \beta_1^{jack} (1)) + exp(\beta_0^{sally} + \beta_1^{sally} (1))}$$` **

&lt;br&gt;

and so on...

---

### Discrete Differences in the Predicted Probability

&lt;br&gt;&lt;br&gt;
If we wanted to calculate the discrete difference for voting for Sally given different values of `\(x\)`, we can simply calculate the difference in probabilities as we did with other models. 

&lt;br&gt;&lt;br&gt;

`$$pr(vote = sally | x = 1) - pr(vote = sally | x = 0)$$`

---

### IIA Assumption 

&lt;br&gt;

- A key assumption for a multinomial choice model is the **independence of irrelevant alternatives assumption (IIA)**. 

- The idea is that the relative probability of existing alternatives (choices) is not affected by changes to the choice set (i.e. adding or removing choices). 

- Adding and dropping a choice doesn't impact the underlying probability of the choices left on the table. 

- If this assumption is violated, estimates will be biased.

---

Say we had commuters deciding between three modes of travel: bus, train, and car.

Each segment represents the probability of choosing that transportation option. Here the probability of choosing any one mode of transportation is roughly equal.

&lt;img src="week-13_ordered-multinomial_ppol561_files/figure-html/unnamed-chunk-28-1.png" style="display: block; margin: auto;" /&gt;

---

Say we had commuters deciding between three modes of travel: bus, train, and car.

Each segment represents the probability of choosing that transportation option. Here it is not.
&lt;br&gt;&lt;br&gt;

&lt;img src="week-13_ordered-multinomial_ppol561_files/figure-html/unnamed-chunk-29-1.png" style="display: block; margin: auto;" /&gt;

---

Say we had commuters deciding between three modes of travel: bus, train, and car.

Now say that we added another option to the table: the chance to ride a &lt;font color ="red"&gt;red&lt;/font&gt; or &lt;font color ="blue"&gt;blue&lt;/font&gt; bus. As long as the addition of these new choices doesn't change the probability of choosing any other choices, IIA is satisfied.
&lt;br&gt;

&lt;img src="week-13_ordered-multinomial_ppol561_files/figure-html/unnamed-chunk-30-1.png" style="display: block; margin: auto;" /&gt;

---

Say we had commuters deciding between three modes of travel: bus, train, and car.

Now say that we added another option to the table: the chance to ride a &lt;font color ="red"&gt;red&lt;/font&gt; or &lt;font color ="blue"&gt;blue&lt;/font&gt; bus. If this is not the case (new bus colors made you more likely to take the bus over the train or car), then &lt;font color = "darkred"&gt; IIA would be violated &lt;/font&gt;. 

&lt;img src="week-13_ordered-multinomial_ppol561_files/figure-html/unnamed-chunk-31-1.png" style="display: block; margin: auto;" /&gt;

---

### Example: Student Support for the Iraq War

Let's use the survey data from of 500 college students in 2002 leading up to the Iraq war. 


&lt;br&gt;

```r
summary(dat)
```

```
##               warsup         dem            female     
##  strongly oppose :168   Min.   :0.000   Min.   :0.000  
##  somewhat oppose : 98   1st Qu.:0.000   1st Qu.:0.000  
##  somewhat support:126   Median :1.000   Median :1.000  
##  strongly support:108   Mean   :0.536   Mean   :0.576  
##                         3rd Qu.:1.000   3rd Qu.:1.000  
##                         Max.   :1.000   Max.   :1.000
```
&lt;br&gt;
&gt; This time rather than treat the outcome as an ordered categories, we'll treat it as **discrete choices**.

---

```r
multi_mod &lt;- nnet::multinom(warsup ~ dem + female,data=dat)
```

```
## # weights:  16 (9 variable)
## initial  value 693.147181 
## iter  10 value 578.220271
## final  value 571.563301 
## converged
```

```r
multi_mod
```

```
## Call:
## nnet::multinom(formula = warsup ~ dem + female, data = dat)
## 
## Coefficients:
##                  (Intercept)        dem       female
## somewhat oppose   0.04525367 -0.4162225 -0.407679600
## somewhat support  1.20336368 -2.5008800  0.007914799
## strongly support  1.59309190 -3.8175076 -0.550507756
## 
## Residual Deviance: 1143.127 
## AIC: 1161.127
```

---

&lt;br&gt;&lt;br&gt;

```r
broom::tidy(multi_mod, 
*           exponentiate = F
            ) %&gt;% 
  mutate_if(is.numeric, function(x) round(x,3))
```

```
## # A tibble: 9 x 6
##   y.level          term        estimate std.error statistic p.value
##   &lt;chr&gt;            &lt;chr&gt;          &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;   &lt;dbl&gt;
## 1 somewhat oppose  (Intercept)    0.045     0.32      0.141   0.888
## 2 somewhat oppose  dem           -0.416     0.325    -1.28    0.2  
## 3 somewhat oppose  female        -0.408     0.261    -1.56    0.119
## 4 somewhat support (Intercept)    1.20      0.274     4.40    0    
## 5 somewhat support dem           -2.50      0.289    -8.66    0    
## 6 somewhat support female         0.008     0.276     0.029   0.977
## 7 strongly support (Intercept)    1.59      0.273     5.83    0    
## 8 strongly support dem           -3.82      0.384    -9.94    0    
## 9 strongly support female        -0.551     0.306    -1.80    0.072
```

---

What is the probability of "Somewhat Supporting" the war given one is a democrat?

--


```r
# Extract the coefficients

B_all &lt;- coefficients(multi_mod)
B_all
```

```
##                  (Intercept)        dem       female
## somewhat oppose   0.04525367 -0.4162225 -0.407679600
## somewhat support  1.20336368 -2.5008800  0.007914799
## strongly support  1.59309190 -3.8175076 -0.550507756
```
&lt;br&gt;

```r
B_so = B_all[1,] # Coefs for "somewhat oppose"
B_ss = B_all[2,] # Coefs for "somewhat support"
B_SS = B_all[3,] # Coefs for "strongly support"
```

---

What is the probability of "Somewhat Supporting" the war given one is a democrat?


```r
# Extract the data 

X = model.matrix(multi_mod)
X
```

```
##     (Intercept) dem female
## 1             1   0      0
## 2             1   0      1
## 3             1   0      1
## 4             1   1      1
## 5             1   0      0
## 6             1   0      0
## 7             1   1      1
## 8             1   1      1
## 9             1   0      0
## 10            1   0      1
## 11            1   0      0
## 12            1   0      0
## 13            1   1      1
## 14            1   1      0
## 15            1   0      1
## 16            1   0      1
## 17            1   1      1
## 18            1   1      1
## 19            1   1      0
## 20            1   0      1
## 21            1   0      0
## 22            1   1      1
## 23            1   1      1
## 24            1   1      1
## 25            1   0      0
## 26            1   1      0
## 27            1   0      1
## 28            1   0      0
## 29            1   1      1
## 30            1   0      0
## 31            1   0      1
## 32            1   1      1
## 33            1   1      1
## 34            1   1      1
## 35            1   0      0
## 36            1   0      0
## 37            1   1      1
## 38            1   1      1
## 39            1   0      1
## 40            1   1      0
## 41            1   1      1
## 42            1   1      1
## 43            1   0      1
## 44            1   1      1
## 45            1   1      1
## 46            1   0      1
## 47            1   1      1
## 48            1   0      1
## 49            1   0      1
## 50            1   1      0
## 51            1   0      0
## 52            1   1      0
## 53            1   1      1
## 54            1   0      1
## 55            1   1      0
## 56            1   0      0
## 57            1   1      1
## 58            1   0      0
## 59            1   0      0
## 60            1   1      1
## 61            1   1      1
## 62            1   1      1
## 63            1   0      1
## 64            1   1      1
## 65            1   1      0
## 66            1   1      1
## 67            1   0      0
## 68            1   1      1
## 69            1   1      1
## 70            1   1      1
## 71            1   1      1
## 72            1   0      1
## 73            1   1      0
## 74            1   0      0
## 75            1   0      1
## 76            1   0      0
## 77            1   0      0
## 78            1   0      0
## 79            1   0      1
## 80            1   0      0
## 81            1   1      1
## 82            1   0      1
## 83            1   0      0
## 84            1   0      0
## 85            1   1      1
## 86            1   0      0
## 87            1   0      1
## 88            1   1      0
## 89            1   1      1
## 90            1   0      1
## 91            1   0      1
## 92            1   0      1
## 93            1   1      1
## 94            1   1      1
## 95            1   0      1
## 96            1   1      1
## 97            1   1      1
## 98            1   0      1
## 99            1   0      1
## 100           1   0      1
## 101           1   1      1
## 102           1   1      1
## 103           1   0      1
## 104           1   1      1
## 105           1   1      1
## 106           1   0      1
## 107           1   0      0
## 108           1   0      0
## 109           1   0      0
## 110           1   1      1
## 111           1   1      1
## 112           1   1      1
## 113           1   1      0
## 114           1   1      0
## 115           1   1      0
## 116           1   0      0
## 117           1   1      1
## 118           1   1      0
## 119           1   0      0
## 120           1   0      0
## 121           1   0      1
## 122           1   0      1
## 123           1   1      1
## 124           1   1      0
## 125           1   0      1
## 126           1   1      0
## 127           1   0      0
## 128           1   0      0
## 129           1   0      0
## 130           1   1      0
## 131           1   1      1
## 132           1   0      0
## 133           1   1      0
## 134           1   0      1
## 135           1   1      1
## 136           1   1      0
## 137           1   1      1
## 138           1   1      0
## 139           1   1      1
## 140           1   1      1
## 141           1   0      1
## 142           1   1      1
## 143           1   1      1
## 144           1   1      1
## 145           1   0      0
## 146           1   0      0
## 147           1   0      0
## 148           1   0      0
## 149           1   1      1
## 150           1   0      1
## 151           1   0      0
## 152           1   1      1
## 153           1   0      1
## 154           1   1      1
## 155           1   0      1
## 156           1   1      0
## 157           1   1      0
## 158           1   1      1
## 159           1   1      1
## 160           1   0      0
## 161           1   1      0
## 162           1   0      0
## 163           1   1      1
## 164           1   0      1
## 165           1   0      0
## 166           1   0      0
## 167           1   0      0
## 168           1   0      1
## 169           1   1      0
## 170           1   0      0
## 171           1   1      1
## 172           1   1      1
## 173           1   1      1
## 174           1   1      1
## 175           1   1      0
## 176           1   1      1
## 177           1   1      1
## 178           1   0      1
## 179           1   0      0
## 180           1   1      1
## 181           1   1      0
## 182           1   0      1
## 183           1   1      1
## 184           1   0      1
## 185           1   0      1
## 186           1   1      0
## 187           1   1      0
## 188           1   1      1
## 189           1   1      0
## 190           1   1      0
## 191           1   0      0
## 192           1   0      1
## 193           1   1      1
## 194           1   1      1
## 195           1   1      1
## 196           1   1      0
## 197           1   1      0
## 198           1   1      1
## 199           1   1      0
## 200           1   1      0
## 201           1   0      0
## 202           1   0      1
## 203           1   1      1
## 204           1   0      1
## 205           1   1      1
## 206           1   0      1
## 207           1   0      1
## 208           1   0      0
## 209           1   1      1
## 210           1   1      0
## 211           1   1      1
## 212           1   1      1
## 213           1   1      0
## 214           1   0      1
## 215           1   0      0
## 216           1   1      1
## 217           1   1      0
## 218           1   1      0
## 219           1   0      0
## 220           1   0      1
## 221           1   1      0
## 222           1   0      1
## 223           1   1      0
## 224           1   0      1
## 225           1   0      1
## 226           1   0      0
## 227           1   1      1
## 228           1   0      1
## 229           1   0      1
## 230           1   0      1
## 231           1   1      1
## 232           1   1      1
## 233           1   0      0
## 234           1   0      0
## 235           1   1      0
## 236           1   1      0
## 237           1   0      0
## 238           1   0      0
## 239           1   0      0
## 240           1   0      0
## 241           1   0      1
## 242           1   0      0
## 243           1   1      0
## 244           1   0      1
## 245           1   0      1
## 246           1   1      1
## 247           1   1      0
## 248           1   0      1
## 249           1   1      0
## 250           1   1      1
## 251           1   0      1
## 252           1   1      0
## 253           1   0      1
## 254           1   0      1
## 255           1   1      1
## 256           1   1      1
## 257           1   1      0
## 258           1   0      1
## 259           1   1      1
## 260           1   0      1
## 261           1   1      0
## 262           1   0      1
## 263           1   1      0
## 264           1   1      1
## 265           1   1      1
## 266           1   1      1
## 267           1   0      0
## 268           1   1      1
## 269           1   0      1
## 270           1   0      1
## 271           1   0      0
## 272           1   1      1
## 273           1   0      0
## 274           1   0      0
## 275           1   1      0
## 276           1   0      0
## 277           1   0      0
## 278           1   0      1
## 279           1   1      1
## 280           1   0      0
## 281           1   0      0
## 282           1   1      1
## 283           1   0      1
## 284           1   0      0
## 285           1   1      0
## 286           1   0      1
## 287           1   0      0
## 288           1   1      0
## 289           1   1      0
## 290           1   0      1
## 291           1   0      1
## 292           1   1      1
## 293           1   1      1
## 294           1   0      1
## 295           1   1      1
## 296           1   1      0
## 297           1   0      1
## 298           1   1      1
## 299           1   1      1
## 300           1   0      1
## 301           1   1      0
## 302           1   1      0
## 303           1   0      0
## 304           1   0      1
## 305           1   0      0
## 306           1   1      1
## 307           1   0      1
## 308           1   1      0
## 309           1   1      1
## 310           1   1      0
## 311           1   1      0
## 312           1   1      0
## 313           1   1      1
## 314           1   0      0
## 315           1   1      0
## 316           1   0      0
## 317           1   0      1
## 318           1   0      0
## 319           1   0      1
## 320           1   1      1
## 321           1   1      0
## 322           1   1      0
## 323           1   0      0
## 324           1   1      0
## 325           1   0      0
## 326           1   0      0
## 327           1   0      0
## 328           1   0      0
## 329           1   1      1
## 330           1   0      0
## 331           1   1      0
## 332           1   0      1
## 333           1   1      1
## 334           1   1      1
## 335           1   0      1
## 336           1   0      1
## 337           1   0      1
## 338           1   1      1
## 339           1   1      1
## 340           1   0      0
## 341           1   1      1
## 342           1   0      0
## 343           1   1      0
## 344           1   1      1
## 345           1   1      0
## 346           1   0      1
## 347           1   1      1
## 348           1   1      1
## 349           1   0      0
## 350           1   0      1
## 351           1   0      1
## 352           1   1      0
## 353           1   0      0
## 354           1   1      1
## 355           1   1      0
## 356           1   0      0
## 357           1   1      0
## 358           1   1      1
## 359           1   0      1
## 360           1   1      1
## 361           1   0      0
## 362           1   1      1
## 363           1   1      1
## 364           1   0      0
## 365           1   0      1
## 366           1   1      1
## 367           1   1      0
## 368           1   0      1
## 369           1   0      0
## 370           1   1      1
## 371           1   0      0
## 372           1   1      0
## 373           1   0      0
## 374           1   0      0
## 375           1   1      1
## 376           1   0      0
## 377           1   1      1
## 378           1   1      1
## 379           1   0      0
## 380           1   1      1
## 381           1   0      1
## 382           1   0      1
## 383           1   1      1
## 384           1   1      1
## 385           1   1      0
## 386           1   1      1
## 387           1   0      1
## 388           1   1      1
## 389           1   1      1
## 390           1   1      1
## 391           1   1      1
## 392           1   1      0
## 393           1   1      1
## 394           1   0      1
## 395           1   1      0
## 396           1   0      0
## 397           1   0      1
## 398           1   1      0
## 399           1   0      0
## 400           1   0      0
## 401           1   1      1
## 402           1   1      1
## 403           1   0      1
## 404           1   1      0
## 405           1   1      1
## 406           1   1      1
## 407           1   1      1
## 408           1   1      0
## 409           1   0      0
## 410           1   0      0
## 411           1   0      1
## 412           1   1      1
## 413           1   1      1
## 414           1   1      1
## 415           1   1      1
## 416           1   0      1
## 417           1   1      1
## 418           1   0      0
## 419           1   1      1
## 420           1   0      1
## 421           1   0      1
## 422           1   1      1
## 423           1   1      0
## 424           1   0      0
## 425           1   0      0
## 426           1   0      1
## 427           1   1      0
## 428           1   1      0
## 429           1   1      0
## 430           1   1      0
## 431           1   1      1
## 432           1   0      0
## 433           1   1      1
## 434           1   0      1
## 435           1   0      0
## 436           1   0      0
## 437           1   0      1
## 438           1   1      0
## 439           1   0      0
## 440           1   1      1
## 441           1   1      1
## 442           1   0      1
## 443           1   0      0
## 444           1   1      1
## 445           1   0      1
## 446           1   1      1
## 447           1   0      0
## 448           1   0      0
## 449           1   1      1
## 450           1   1      1
## 451           1   1      1
## 452           1   1      1
## 453           1   0      0
## 454           1   1      1
## 455           1   1      0
## 456           1   0      0
## 457           1   1      0
## 458           1   1      1
## 459           1   1      1
## 460           1   0      1
## 461           1   0      0
## 462           1   0      1
## 463           1   0      0
## 464           1   1      1
## 465           1   0      1
## 466           1   1      1
## 467           1   1      1
## 468           1   0      1
## 469           1   1      1
## 470           1   1      1
## 471           1   0      0
## 472           1   1      0
## 473           1   1      1
## 474           1   1      0
## 475           1   1      1
## 476           1   1      0
## 477           1   0      1
## 478           1   1      1
## 479           1   1      1
## 480           1   1      0
## 481           1   1      0
## 482           1   1      0
## 483           1   1      0
## 484           1   0      1
## 485           1   1      1
## 486           1   1      0
## 487           1   0      1
## 488           1   1      1
## 489           1   1      1
## 490           1   0      0
## 491           1   0      1
## 492           1   1      1
## 493           1   1      1
## 494           1   0      1
## 495           1   0      1
## 496           1   0      1
## 497           1   1      0
## 498           1   1      0
## 499           1   0      1
## 500           1   1      0
## attr(,"assign")
## [1] 0 1 2
```


---

What is the probability of "Somewhat Supporting" the war given one is a democrat?


```r
# Manipulate one value 
X[,2] = 1 # Respondent is a democrat
```

Calculate the predicted probability for the "Somwhat support" choice category. 

```r
# Denominator of all options
denominator &lt;- 1 + exp(X%*%B_so) + exp(X%*%B_ss) + exp(X%*%B_SS)

# Probability of "Somewhat Support"
pr_some_supp = exp(X%*%B_ss)/denominator

# Average across all observations
mean(pr_some_supp)
```

```
## [1] 0.1442044
```

---

What is the probability of "Somewhat Supporting" the war given one is _not_ a democrat?


```r
# Manipulate one value 
*X[,2] = 0 # Respondent is not a democrat
```

Calculate the predicted probability for the "Somwhat support" choice category. 

```r
# Denominator of all options
denominator &lt;- 1 + exp(X%*%B_so) + exp(X%*%B_ss) + exp(X%*%B_SS)

# Probability of "Somewhat Support"
pr_some_supp = exp(X%*%B_ss)/denominator

# Average across all observations
mean(pr_some_supp)
```

```
## [1] 0.3823005
```

---

&lt;br&gt;&lt;br&gt;
The discrete difference between "Somewhat Supporting" the war given one...

&lt;br&gt;

- ...is a democrat = **14.4%**

- ...is _not_ a democrat = **38.2%**

&lt;br&gt;&lt;br&gt;
The probability of "Somewhat Supporting" the war **decreases by 23.8%** if the respondent is a Democrat verse if he or she is not.

---

### Simulating Confidence Intervals


```r
require(obsval) # Load the package

# Re-estimate the model using obsval
multi_mod2 &lt;- 
  obsval(warsup ~ dem + female,
         data = dat,
         ci = .95,
         n.draws = 1000,
*        baseline.category = "strongly oppose",
         reg.model = "mlogit",
         effect.var = "dem",
         effect.vals = c(0,1))
```

```
## # weights:  16 (9 variable)
## initial  value 693.147181 
## iter  10 value 578.220271
## final  value 571.563301 
## converged
```

---

### Simulating Confidence Intervals


```r
round(multi_mod2$effect_sum,3)
```

```
##                    2.5%   Mean  97.5%
## somewhat oppose   0.111  0.193  0.271
## somewhat support -0.368 -0.237 -0.106
## strongly support -0.495 -0.364 -0.242
## strongly oppose   0.350  0.408  0.469
```

---

&lt;br&gt;

```r
multi_mod2$high.ci
```

```
##       somewhat oppose somewhat support strongly support strongly oppose
## var_0       0.1577098        0.5049192       0.53707379       0.1446481
## var_1       0.3494480        0.1928511       0.07815265       0.5741271
```


```r
multi_mod2$means
```

```
##       somewhat oppose somewhat support strongly support strongly oppose
## var_0      0.09629077        0.3819949       0.40829038       0.1134239
## var_1      0.28929165        0.1453674       0.04401585       0.5213251
```


```r
multi_mod2$low.ci
```

```
##       somewhat oppose somewhat support strongly support strongly oppose
## var_0      0.04948838        0.2585046       0.29000900      0.08604159
## var_1      0.23395624        0.1047938       0.02221104      0.46747801
```

---

&lt;img src="week-13_ordered-multinomial_ppol561_files/figure-html/unnamed-chunk-48-1.png" style="display: block; margin: auto;" /&gt;


---

### Compare

.pull-left[
.center[**Ordered**]
&lt;img src="week-13_ordered-multinomial_ppol561_files/figure-html/unnamed-chunk-49-1.png" style="display: block; margin: auto;" /&gt;
]

.pull-right[
.center[**Nominal**]
&lt;img src="week-13_ordered-multinomial_ppol561_files/figure-html/unnamed-chunk-50-1.png" style="display: block; margin: auto;" /&gt;
]
    </textarea>
<style data-target="print-only">@media screen {.remark-slide-container{display:block;}.remark-slide-scaler{box-shadow:none;}}</style>
<script src="https://remarkjs.com/downloads/remark-latest.min.js"></script>
<script src="macros.js"></script>
<script>var slideshow = remark.create({
"highlightStyle": "github",
"countIncrementalSlides": false,
"highlightLines": true
});
if (window.HTMLWidgets) slideshow.on('afterShowSlide', function (slide) {
  window.dispatchEvent(new Event('resize'));
});
(function(d) {
  var s = d.createElement("style"), r = d.querySelector(".remark-slide-scaler");
  if (!r) return;
  s.type = "text/css"; s.innerHTML = "@page {size: " + r.style.width + " " + r.style.height +"; }";
  d.head.appendChild(s);
})(document);

(function(d) {
  var el = d.getElementsByClassName("remark-slides-area");
  if (!el) return;
  var slide, slides = slideshow.getSlides(), els = el[0].children;
  for (var i = 1; i < slides.length; i++) {
    slide = slides[i];
    if (slide.properties.continued === "true" || slide.properties.count === "false") {
      els[i - 1].className += ' has-continuation';
    }
  }
  var s = d.createElement("style");
  s.type = "text/css"; s.innerHTML = "@media print { .has-continuation { display: none; } }";
  d.head.appendChild(s);
})(document);
// delete the temporary CSS (for displaying all slides initially) when the user
// starts to view slides
(function() {
  var deleted = false;
  slideshow.on('beforeShowSlide', function(slide) {
    if (deleted) return;
    var sheets = document.styleSheets, node;
    for (var i = 0; i < sheets.length; i++) {
      node = sheets[i].ownerNode;
      if (node.dataset["target"] !== "print-only") continue;
      node.parentNode.removeChild(node);
    }
    deleted = true;
  });
})();
// adds .remark-code-has-line-highlighted class to <pre> parent elements
// of code chunks containing highlighted lines with class .remark-code-line-highlighted
(function(d) {
  const hlines = d.querySelectorAll('.remark-code-line-highlighted');
  const preParents = [];
  const findPreParent = function(line, p = 0) {
    if (p > 1) return null; // traverse up no further than grandparent
    const el = line.parentElement;
    return el.tagName === "PRE" ? el : findPreParent(el, ++p);
  };

  for (let line of hlines) {
    let pre = findPreParent(line);
    if (pre && !preParents.includes(pre)) preParents.push(pre);
  }
  preParents.forEach(p => p.classList.add("remark-code-has-line-highlighted"));
})(document);</script>

<script>
(function() {
  var links = document.getElementsByTagName('a');
  for (var i = 0; i < links.length; i++) {
    if (/^(https?:)?\/\//.test(links[i].getAttribute('href'))) {
      links[i].target = '_blank';
    }
  }
})();
</script>

<script>
slideshow._releaseMath = function(el) {
  var i, text, code, codes = el.getElementsByTagName('code');
  for (i = 0; i < codes.length;) {
    code = codes[i];
    if (code.parentNode.tagName !== 'PRE' && code.childElementCount === 0) {
      text = code.textContent;
      if (/^\\\((.|\s)+\\\)$/.test(text) || /^\\\[(.|\s)+\\\]$/.test(text) ||
          /^\$\$(.|\s)+\$\$$/.test(text) ||
          /^\\begin\{([^}]+)\}(.|\s)+\\end\{[^}]+\}$/.test(text)) {
        code.outerHTML = code.innerHTML;  // remove <code></code>
        continue;
      }
    }
    i++;
  }
};
slideshow._releaseMath(document);
</script>
<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
(function () {
  var script = document.createElement('script');
  script.type = 'text/javascript';
  script.src  = 'https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-MML-AM_CHTML';
  if (location.protocol !== 'file:' && /^https?:/.test(script.src))
    script.src  = script.src.replace(/^https?:/, '');
  document.getElementsByTagName('head')[0].appendChild(script);
})();
</script>
  </body>
</html>
