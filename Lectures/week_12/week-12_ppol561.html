<!DOCTYPE html>
<html lang="" xml:lang="">
  <head>
    <title> PPOL561 | Accelerated Statistics for Public Policy II  Week 12       Generalized Linear Models</title>
    <meta charset="utf-8" />
    <meta name="author" content="  Prof. Eric Dunford  ◆  Georgetown University  ◆  McCourt School of Public Policy  ◆  eric.dunford@georgetown.edu" />
    <link rel="stylesheet" href="gu-theme.css" type="text/css" />
  </head>
  <body>
    <textarea id="source">
class: center, middle, inverse, title-slide

# <font class = "title-panel"> PPOL561 | Accelerated Statistics for Public Policy II</font> <font size=6, face="bold"> Week 12 </font> <br> <br> <font size=100, face="bold"> Generalized Linear Models </font>
### <font class = "title-footer">  Prof. Eric Dunford  ◆  Georgetown University  ◆  McCourt School of Public Policy  ◆  <a href="mailto:eric.dunford@georgetown.edu" class="email">eric.dunford@georgetown.edu</a></font>

---




layout: true

&lt;div class="slide-footer"&gt;&lt;span&gt; 
PPOL561 | Accelerated Statistics for Public Policy II

&amp;emsp;&amp;emsp;&amp;emsp;&amp;emsp;&amp;emsp;&amp;emsp;&amp;emsp;&amp;emsp;&amp;emsp;

Week 12 &lt;!-- Week of the Footer Here --&gt;

&amp;emsp;&amp;emsp;&amp;emsp;&amp;emsp;&amp;emsp;&amp;emsp;&amp;emsp;&amp;emsp;&amp;emsp;&amp;emsp;&amp;emsp;&amp;emsp;

GLMs &lt;!-- Title of the lecture here --&gt;

&lt;/span&gt;&lt;/div&gt; 

---
class: outline

# Outline for Today 

&lt;br&gt;

- **_Linear Probability Models_** and their discontents

&lt;br&gt;

- **_Maximum Likelihood Estimation_** for binary responses

&lt;br&gt;

- It's all about the **_Substantive Effects_** and generating estimates of **_Uncertainty_** around substantive predictions.

---

# Limited dependent variables

![:space 5]

- **Binary** – vote or not; go to war or not

- **Multinomial** – voted for Clinton, Trump, Johnson, or Stein

- **Ordinal** – strongly oppose war, somewhat oppose war, somewhat support war, strongly support war

- **Counts** – number of bills passed; number of conflicts; number of organizations joined; number of puppies hugged

- **Duration** – time to adopt full suffrage; time to bill passage; time to end of regime

---

class: newsection

# Binary Dependent Variables

---

## The problem

When we have a binary outcome (0/1), we can use OLS to estimate the relationship. This is known as a **linear probability model** (LPM).  

--

We start with the usual equation: 

`$$y_i = \beta_0 + \beta_1 x_i + \epsilon_i$$`
Here `\(y \in [0,1]\)`
   
- The probability of "success": `\(pr(y_i = 1) = p_i\)`
- The probability of "failure": `\(pr(y_i = 0) = 1 - p_i\)` 

--

`\(y\)` follows a **Bernoulli** distribution

$$ E(y_i) = 0 \times (1-p_i) + 1 \times p_i = p_i$$
$$ E(y_i | x_i) = \beta_0 + \beta_1 x_i = p_i $$


---

## The problem

&lt;img src="week-12_ppol561_files/figure-html/unnamed-chunk-1-1.png" style="display: block; margin: auto;" /&gt;

--

.center[
&lt;table&gt;
 &lt;thead&gt;
  &lt;tr&gt;
   &lt;th style="text-align:left;"&gt; term &lt;/th&gt;
   &lt;th style="text-align:right;"&gt; estimate &lt;/th&gt;
   &lt;th style="text-align:right;"&gt; std.error &lt;/th&gt;
   &lt;th style="text-align:right;"&gt; statistic &lt;/th&gt;
   &lt;th style="text-align:right;"&gt; p.value &lt;/th&gt;
  &lt;/tr&gt;
 &lt;/thead&gt;
&lt;tbody&gt;
  &lt;tr&gt;
   &lt;td style="text-align:left;"&gt; (Intercept) &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; 0.690 &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; 0.034 &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; 20.222 &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; 0 &lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
   &lt;td style="text-align:left;"&gt; x &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; 0.328 &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; 0.037 &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; 8.756 &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; 0 &lt;/td&gt;
  &lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
]

---

## The problem

&lt;img src="week-12_ppol561_files/figure-html/unnamed-chunk-3-1.png" style="display: block; margin: auto;" /&gt;



.center[
&lt;table&gt;
 &lt;thead&gt;
  &lt;tr&gt;
   &lt;th style="text-align:left;"&gt; term &lt;/th&gt;
   &lt;th style="text-align:left;"&gt; estimate &lt;/th&gt;
   &lt;th style="text-align:right;"&gt; std.error &lt;/th&gt;
   &lt;th style="text-align:right;"&gt; statistic &lt;/th&gt;
   &lt;th style="text-align:right;"&gt; p.value &lt;/th&gt;
  &lt;/tr&gt;
 &lt;/thead&gt;
&lt;tbody&gt;
  &lt;tr&gt;
   &lt;td style="text-align:left;"&gt; (Intercept) &lt;/td&gt;
   &lt;td style="text-align:left;"&gt; 69% &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; 0.034 &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; 20.222 &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; 0 &lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
   &lt;td style="text-align:left;"&gt; x &lt;/td&gt;
   &lt;td style="text-align:left;"&gt; 32.8% &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; 0.037 &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; 8.756 &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; 0 &lt;/td&gt;
  &lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
]

---

## The problem

- OLS may produce **non-sense predictions** (recall that OLS assumes continuous interval level data)

--

  + But we can **recode**
  
&lt;img src="week-12_ppol561_files/figure-html/unnamed-chunk-5-1.png" style="display: block; margin: auto;" /&gt;


---

## The problem

- OLS may produce **non-sense predictions** (recall that OLS assumes continuous interval level data)

- Disturbances are &lt;u&gt;_not_&lt;/u&gt; normally distributed; they follow the Bernoulli distribution.

--

&lt;img src="week-12_ppol561_files/figure-html/unnamed-chunk-6-1.png" style="display: block; margin: auto;" /&gt;



---

## The problem

- OLS may produce **non-sense predictions** (recall that OLS assumes continuous interval level data)

- Disturbances are &lt;u&gt;_not_&lt;/u&gt; normally distributed, they follow the Bernoulli distribution.

&lt;img src="week-12_ppol561_files/figure-html/unnamed-chunk-7-1.png" style="display: block; margin: auto;" /&gt;

---

## The problem

- OLS may produce **non-sense predictions** (recall that OLS assumes continuous interval level data)

- Disturbances are &lt;u&gt;_not_&lt;/u&gt; normally distributed, they follow the Bernoulli distribution.

- `\(y_i\)` is distributed with mean `\(p_i\)` and variance 
	`\(p_i(1-p_i)\)`. `\(p_i\)` is a function of `\(x\)`, so we have **heteroscedasticity** with variance related to the `\(x\)`.

	 + But can use weighted least squares to adjust for this. 

--

- Linear assumption is suspect

- Goodness-of-fit measures are even less helpful

---

## The solution

![:space 5]

**_Use a model that fits the functional form of the outcome variable_**!

![:space 5]

Two ways of thinking about doing this (both point to the same modeling strategy):

![:space 3]

- **_Latent variables_**

- **_Generalized linear models_**

---

### Latent variable way of thinking

- Recall we only observe `\(y = 1\)` (success) and `\(y = 0\)` (failure).

--

- But assume there is some underlying process with a continuous distribution `\(y^*\)`, such that

`$$y_i^* = \beta_0 + \beta_1 x_i + \epsilon_i$$`

--

- where `\(y_i = 1\)` (success) if `\(y_i^* &gt; \tau\)` (some threshold) and `\(y_i = 0\)` otherwise (failure)

`$$y_i = \begin{cases}1~~~\text{if}~y^* &gt; \tau \\ 0~~~\text{if}~y^* \le \tau \end{cases}$$`

- The threshold ( `\(\tau\)` ) is some point on the latent distribution. For simplicity, we can set `\(\tau = 0\)`. 

---

&lt;img src="week-12_ppol561_files/figure-html/unnamed-chunk-8-1.png" style="display: block; margin: auto;" /&gt;

---


- `\(y^*\)` is continuous, so we avoid the problems encountered using LPM. However, `\(y^*\)` is unobserved, so we cannot estimate it using OLS. 

- Instead we use Maximum Likelihood Estimations, which requires assumptions about the **distribution of the errors**. 

--

- Can write the probability of success `\(p_i\)` as (recall `\(\tau = 0\)`)

`$$p_i = pr(\beta_0 + \beta_1 x_i + \epsilon_i &gt; 0)$$`

`$$p_i = pr(\epsilon_i &gt; -\beta_0 - \beta_1 x_i)$$`
`$$p_i = pr(\epsilon_i &lt; \beta_0 + \beta_1 x_i)$$`

`$$p_i = F(\beta_0 + \beta_1 x_i)$$` 

--

- Where `\(F(.)\)` is the cumulative density function (CDF) of `\(\epsilon\)`. We can evaluate the CDF of `\(\epsilon_i\)` at `\(\beta_0 + \beta_1 x_i\)` to get the probability of success. But we need to pick a form for `\(F(.)\)`. 

---

&lt;img src="week-12_ppol561_files/figure-html/unnamed-chunk-9-1.png" style="display: block; margin: auto;" /&gt;

---



The **logistic (logit)** and **Normal (probit)** distributions are used frequently.
&lt;br&gt;&lt;br&gt;&lt;br&gt;&lt;br&gt;
.center[
|   | CDF | PDF  |
|---|-----|---|---|
| **Normal** | `\(\Phi(\epsilon) = \int^\epsilon_{-\infty} \frac{1}{(\sqrt{2 \pi})} e^{-\frac{t^2}{2}} dt~~\)` | `\(\phi(\epsilon) = \frac{1}{(\sqrt{2 \pi})} e^{-\frac{\epsilon^2}{2}}\)` | 
| **Logistic** | `\(\Lambda(\epsilon) = \frac{e^\epsilon}{1 + e^\epsilon}\)` | `\(\lambda(\epsilon) = \frac{e^\epsilon}{(1 + e^\epsilon)^2}\)` |
]


---

&lt;img src="week-12_ppol561_files/figure-html/unnamed-chunk-10-1.png" style="display: block; margin: auto;" /&gt;

---

### Generalized Linear Model way of thinking

- Rather than thinking in terms of a latent variable, let's specify a relationship between `\(x\)` and the probability of an event such that `\(pr(y_i = 1 | x)\)` is a function that ranges from `\(-\infty\)` to `\(\infty\)`. 

--

- First, we transform the outcome into the **odds**

`$$\frac{pr(y_i = 1 | x)}{pr(y_i = 0 | x)} = \frac{pr(y_i = 1 | x)}{1 - pr(y_i = 1 | x)}$$`

- **Odds** indicate how often something happens relative to how often it doesn't happen. 

--

- The **log of the odds** which ranges from `\(-\infty\)` to `\(\infty\)`.

`$$ln\begin{bmatrix}\frac{pr(y_i = 1 | x)}{1 - pr(y_i = 1 | x)}\end{bmatrix}  = X \beta$$`

---

### Generalized Linear Model way of thinking

- This is equivalent to the logit model we've already discussed.

`$$pr(y_i = 1|X) = \frac{e^{X\beta}}{1+e^{X\beta}} = \frac{1}{1+e^{-X\beta}}$$`

--

- Other probability models can be constructed by choosing functions of `\(X\beta\)` that range from 0 to 1. As we've seen, CDFs have this property.

- The CDF of the standard normal distribution results in the probit model.

`$$pr(y_i = 1|X) = \int^{X\beta}_{-\infty} \frac{1}{(\sqrt{2 \pi})} e^{-\frac{t^2}{2}} dt = \Phi(X\beta)$$`

---

In essence, our function `\(F(\cdot)\)` (which is known as a **link function**) maps our linear combination of independent variables ( `\(X\beta\)` ) onto a probability space (ranging from 0 to 1).

$$ F(X\beta) \mapsto [0,1]$$

.center[&lt;img src="Figures/lin-to-pr-space.gif", width=400 &gt;]

---

Recall that the outcome `\(y_i\)` follows a **Bernoulli distribution**.

`$$y_i \sim bernoulli(p_i)$$`
`$$y_i \sim p_i^{y_i}(1-p_i)^{1-y_i}$$`

--

&lt;br&gt;

We can plug in our linear combination of predictors into this probability distribution. All we need to make sure is that our predictors map to a probablity space (this is our "probability model") 

$$ \pi_i =\Phi(\beta_0 + \beta_1 x_i)  $$
&lt;br&gt;
`$$y_i \sim bernoulli(\pi_i)$$`

`$$y_i \sim \pi_i^{y_i}(1-\pi_i)^{1-y_i}$$`

---


```r
set.seed(123)
N = 100
x &lt;- rnorm(N)            # random variable
y_star &lt;- -1 + .5*x       # latent variable as a linear combination

# convert to a probability space
*pr &lt;- pnorm(y_star)

# Drop the probability into a bernoulli dist. (0 or 1)
*y &lt;- rbinom(N, size=1, prob = pr)

# Gather as dataset
dat1 &lt;- tibble(x,y_star,pr,y)
head(dat1)
```

```
## # A tibble: 6 x 4
##         x y_star    pr     y
##     &lt;dbl&gt;  &lt;dbl&gt; &lt;dbl&gt; &lt;int&gt;
## 1 -0.560  -1.28  0.100     0
## 2 -0.230  -1.12  0.132     1
## 3  1.56   -0.221 0.413     1
## 4  0.0705 -0.965 0.167     0
## 5  0.129  -0.935 0.175     0
## 6  1.72   -0.142 0.443     1
```


---


```r
pairs(dat1,col="steelblue")
```

&lt;img src="week-12_ppol561_files/figure-html/unnamed-chunk-12-1.png" style="display: block; margin: auto;" /&gt;

--


```r
mod = glm(y~x,data=dat1,family=binomial(link = "probit"))
broom::tidy(mod)
```

```
## # A tibble: 2 x 5
##   term        estimate std.error statistic  p.value
##   &lt;chr&gt;          &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;    &lt;dbl&gt;
## 1 (Intercept)   -1.02      0.166     -6.16 7.32e-10
## 2 x              0.532     0.180      2.96 3.05e- 3
```

---

class:newsection

## Maximum Likelihood Estimation

---

### The Steps

- (1) Construct a probability model 

$$ \pi = \Phi(X\beta)$$

--

- (2) Find the probability of the data given the parameters by constructing the likelihood function. 

`$$pr(y|\beta) = \mathcal{L}(\beta|y) = \prod^n_{i=1}\pi_i^{y_i}(1-\pi_i)^{1-y_i}$$`

--

- (3) Take the log of the likelihood and simplify

`$$\log \mathcal{L}(\beta | y) = \sum_{i = 1}^n y_i \log \pi_i + \sum_{i = 1}^n (1 - y_i)\log(1 - \pi_i)$$`

- (4) Maximize the log likelihood function 

---

### Optimize

`$$\log \mathcal{L}(\beta | y) = \sum_{i = 1}^n y_i \log [\Phi(X_i\beta)] + \sum_{i = 1}^n (1 - y_i)\log\left[1 - \Phi(X_i\beta)\right]$$`

Let's construct a log likelihood function for a probit model.


```r
ll.probit &lt;- function(beta, y, X) {
  
  # Convert to a probability sapce
  pr &lt;- pnorm(X%*%beta)
  
  # Caluculate the log-likelihood for every observation
  loglik &lt;- sum(y*log(pr)) + sum((1 - y)*log(1 - pr))
  
  # Return the log-likelihood
  return(loglik)
}
```

--

How might we change this to a logit function?

---

### Optimize

&lt;br&gt;

- Many ways to **optimize an objective function**:

  + Nelder and Mead (default for `optim()`)
  + Newton-Raphson
  + BFGS (quasi-Newton-Raphson)
  + Gradient Descent
  + BFGS 
  + SANN (simulated annealing)
  + And many more...
  
&lt;br&gt;
  
- `R` holds an `optim()` function has many common optimization algorithms contained within it. 

---


```r
# optimization function to calculate maximum likelihood 
probit &lt;- function(y, X) {
  
  # Starting values
  init.par &lt;- rep(0, ncol(X))
  
  # optimizer
  est &lt;- optim(par = init.par,  # starting points
               fn = ll.probit,   # function to optimize
               y = y, X = X,    # Data values
               control = list(fnscale = -1), # -1 == maximize
               hessian = TRUE)  # return the hessian
  
  beta.hat &lt;- est$par # Estimates for beta
  cov &lt;- solve(-est$hessian) # inverse hess == vcov matrix
  
  # Return as list
  res &lt;- list(beta.hat = beta.hat,
              cov = cov)
  return(res)
}
```

---


```r
# Recall our simulated model
mod &lt;- glm(y~x,data=dat1,family=binomial('probit'))
broom::tidy(mod)
```

```
## # A tibble: 2 x 5
##   term        estimate std.error statistic  p.value
##   &lt;chr&gt;          &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;    &lt;dbl&gt;
## 1 (Intercept)   -1.02      0.166     -6.16 7.32e-10
## 2 x              0.532     0.180      2.96 3.05e- 3
```

--


```r
y &lt;- dat1$y
X &lt;- cbind(1,dat1$x) # 1s for the constant
*mod2 &lt;- probit(y,X) # Our homemade MLE optimizer

# Estimates
mod2$beta.hat
```

```
## [1] -1.021831  0.531962
```


```r
# Standard Errors
sqrt(diag(mod2$cov))
```

```
## [1] 0.1652268 0.1777183
```

---

class:newsection

### Substantive Effects

---

### What is the effect of `x` on `y`?

- In OLS, calculating the marginal effect is easy. 

`$$y = \beta_0 + \beta_1 x$$`

`$$\frac{dy}{dx} = \beta_1$$`

--

- In MLE, less so... 

`$$pr(y=1|x) = \Phi(\beta_0 + \beta_1 x)$$`
`$$\frac{dy}{dx} pr(y=1|x) = \phi(\beta_0 + \beta_1 x)\beta_1$$`

where `\(\phi(\cdot)\)` (the Normal PDF) denotes the first derivative of Normal CDF.

--

&gt; Recall the **chain rule**: `\(F(x) = f(g(x)) \to F'(x) = f'(g(x))g'(x)\)` 

---

### What is the effect of `x` on `y`?

- The effect depends on `\(\Phi(\cdot)\)`, the coefficient on `\(x\)` (i.e. `\(\beta\)`), and the values of &lt;u&gt;**_all the other variables and their respective coefficients_**&lt;/u&gt;.

--

- In other words the **_model is inherently interactive in all of the variables_**. The effect of a unit change in `\(x\)` is not constant. It depends where we are on the curve.

--

- The output from the model merely tells us:
  + The **sign** of the effect of `\(x\)` on the probability of success
  + Overall **statistical significance** of the effect
  
- We need to **transform** them in order to determine **_substantive significance_**.
  
---
  
&lt;img src="week-12_ppol561_files/figure-html/unnamed-chunk-19-1.png" style="display: block; margin: auto;" /&gt;

---

### Marginal Effects

We can easily calculate and plot the first derivative to get a sense of the marginal effects.


```r
b = coefficients(mod)
X = model.matrix(mod)
head(X)
```

```
##   (Intercept)           x
## 1           1 -0.56047565
## 2           1 -0.23017749
## 3           1  1.55870831
## 4           1  0.07050839
## 5           1  0.12928774
## 6           1  1.71506499
```



```r
*me = dnorm(X%*%b)*b[2]
```

---


```r
dat1 %&gt;% 
  mutate(me=me) %&gt;%
ggplot(aes(x,me)) +
  geom_line() +
  labs(y="dy/dx")
```

&lt;img src="week-12_ppol561_files/figure-html/unnamed-chunk-22-1.png" style="display: block; margin: auto;" /&gt;

--

But marginal effects don't always offer an intuitive interpretation of the results. 

---

### Predicted probabilities

- Interest is in the **_substantive significance of the effect_** 

  + Coefficients reveal little
  
  + Probabilities are easier to understand than log-odds
  
  + The effects depend on where you start on the curve and on the values of all of the other independent variables in the model
  
--

- Approaches for substantive interpretation using predicted probabilities

  + **_Average Case Approach_**: set all other values to means (modes for dummies)
  
  + **_Observed Value Approach_**: set all other values to their observed values

---

### Predicted probabilities

- Interest is in the **_substantive significance of the effect_** 

  + Coefficients reveal little
  
  + Probabilities are easier to understand than log-odds
  
  + The effects depend on where you start on the curve and on the values of all of the other independent variables in the model
 
&lt;br&gt;
 
- **Observed Value Approach** provides a closer connection between the results and the theory and research design (_Hanmer and Kalkan 2013_)

- The goal is to estimate the **_average effect_**, _NOT_ the **_effect for the average case._** 

---

### Example Data 



- Post-election survey taken after the 2000 presidential election (roughly 2188 responses in this data. Non-response on the voting outcome are dropped.).

- Question: does how many days out voter registration closes impact an individual's propensity to vote?



```r
head(elect)
```

```
## # A tibble: 6 x 7
##    vote close homeown edu7cat   age marriage  male
##   &lt;dbl&gt; &lt;dbl&gt;   &lt;dbl&gt;   &lt;dbl&gt; &lt;dbl&gt;    &lt;dbl&gt; &lt;dbl&gt;
## 1     0    14       1       1    44        0     1
## 2     1    25       0       1    64        0     0
## 3     1    29       1       1    68        1     0
## 4     1    30       1       1    65        1     0
## 5     0    30       1       1    46        1     1
## 6     1    29       0       1    59        1     1
```


---

### Example Data

- Post-election survey taken after the 2000 presidential election (roughly 2188 responses in this data. Non-response on the voting outcome are dropped.).

- Question: does how many days out voter registration closes impact an individual's propensity to vote?


```r
mod2 &lt;- glm(vote~close+male+age,
            data=elect,family=binomial('probit'))
broom::tidy(mod2) %&gt;% 
  mutate_if(is.numeric,function(x) round(x,3))
```

```
## # A tibble: 4 x 5
##   term        estimate std.error statistic p.value
##   &lt;chr&gt;          &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;   &lt;dbl&gt;
## 1 (Intercept)   -0.288     0.106    -2.71    0.007
## 2 close         -0.006     0.003    -2.20    0.028
## 3 male          -0.004     0.057    -0.073   0.942
## 4 age            0.019     0.002    11.2     0
```

---

Who are we trying to predict?


```r
b &lt;- coefficients(mod2)

z &lt;- b[1]    + # intercept
     b[2]*15 + # close
     b[3]*1  + # male
     b[4]*35   # age

pnorm(z) # probability of voting
```

```
## (Intercept) 
##   0.6103261
```


--


```r
z &lt;- b[1]    + # intercept
     b[2]*0  + # close
     b[3]*0  + # male
     b[4]*76   # age

pnorm(z) # probability of voting
```

```
## (Intercept) 
##   0.8775209
```


---

### Predicted probabilities


&lt;br&gt;
&lt;br&gt;

**The Recipe**:

1. Isolate a single (discrete or continuous) variable.

2. Manipulate only that value.

3. Calculate the prediction and average across the observations.

4. Compare to some other condition.

---


```r
X = model.matrix(mod2)
head(X)
```

```
##   (Intercept) close male age
## 1           1    14    1  44
## 2           1    25    0  64
## 3           1    29    0  68
## 4           1    30    0  65
## 5           1    30    1  46
## 6           1    29    1  59
```


```r
# Alter a condition (Register at the poll)
X[,2] = 0
```



```r
# Predict
prob_sameday = pnorm(X%*%b)
```



```r
# average
mean(prob_sameday)
```

```
## [1] 0.7102054
```


---


```r
X = model.matrix(mod2)
head(X)
```

```
##   (Intercept) close male age
## 1           1    14    1  44
## 2           1    25    0  64
## 3           1    29    0  68
## 4           1    30    0  65
## 5           1    30    1  46
## 6           1    29    1  59
```


```r
# Alter a condition (Registration closes 30 days out)
X[,2] = 30
```



```r
# Predict
prob_30days = pnorm(X%*%b)
```



```r
# average
mean(prob_30days)
```

```
## [1] 0.6451213
```

---

## Discrete Difference 

We can compare two predictions (where only one data value is altered) by differencing the predictions.

--


```r
mean(prob_30days) - mean(prob_sameday)
```

```
## [1] -0.06508412
```

--

&lt;br&gt;
When the polls close 30 days prior to election day the likelihood of one voting decreases by 6.5%!

--

&lt;br&gt;
What's one issue with this conclusion?

--

![:space 5]

.center[**_No estimate of uncertainty!_**]

---

## Monte Carlo Simulation

- We need to calculate uncertainty around our predictions. _But_ we can’t directly calculate our confidence for any one prediction, so we need to simulate it.

--

- The aim is to:

  + **simulate** all the possible configurations of the **coefficients**, 
  + **compute** the resulting **predictions** for each simulated beta, and then 
  + **calculate** the 95% **confidence interval** using the resulting distribution.

--

- To do this, we can:
  + take repeated random draws from a simulated distribution of the model coefficient
  + use information from the model regarding the bounds of this distribution (central tendancy, covariance) 




---


```r
# Simulate the coefficents
betas = coefficients(mod2) # coeficient estimates
sigma = vcov(mod2) # variance-covariance matrix
sigma 
```

```
##               (Intercept)         close          male           age
## (Intercept)  0.0112984032 -1.831202e-04 -1.801604e-03 -1.262345e-04
## close       -0.0001831202  8.383407e-06  3.785560e-06 -1.576920e-07
## male        -0.0018016042  3.785560e-06  3.213478e-03  4.546818e-06
## age         -0.0001262345 -1.576920e-07  4.546818e-06  2.909608e-06
```

--

Simulate the beta coefficents using a multinormal distribution. We can use the `mvrnorm()` function to take random draws from this distribution using the `MASS` package.


```r
set.seed(1234) # Set a seed for reproducibility
*sim_betas = MASS::mvrnorm(n=1000,mu=betas,Sigma = sigma)
```


```r
dim(sim_betas)
```

```
## [1] 1000    4
```

---


```r
betas
```

```
##  (Intercept)        close         male          age 
## -0.288148097 -0.006378627 -0.004149490  0.019089884
```



```r
as_tibble(sim_betas) %&gt;% 
  gather(param,betas) %&gt;% 
  ggplot(aes(betas)) +
  geom_histogram(color="white",alpha=.5,fill="darkred") +
  facet_wrap(~param,ncol=4,scales="free")
```

&lt;img src="week-12_ppol561_files/figure-html/unnamed-chunk-41-1.png" style="display: block; margin: auto;" /&gt;

---


```r
X = model.matrix(mod2)
head(X,3)
```

```
##   (Intercept) close male age
## 1           1    14    1  44
## 2           1    25    0  64
## 3           1    29    0  68
```

--


```r
X[,2] = 0 # same day voting
```

--

Calculate the average predicted value for every combination of the coefficents


```r
n_sims = nrow(sim_betas)
probs_sameday &lt;- rep(0,n_sims)
for ( i in 1:n_sims){
  probs_sameday[i] &lt;- mean(pnorm(X%*%sim_betas[i,]))
}
*quantile(probs_sameday,probs = c(.025,.975))
```

```
##      2.5%     97.5% 
## 0.6645399 0.7521849
```

---


```r
X = model.matrix(mod2)
head(X,3)
```

```
##   (Intercept) close male age
## 1           1    14    1  44
## 2           1    25    0  64
## 3           1    29    0  68
```



```r
X[,2] = 30 # closes 30 days out
```


Calculate the average predicted value for every combination of the coefficents


```r
n_sims = nrow(sim_betas)
probs_30days &lt;- rep(0,n_sims)
for ( i in 1:n_sims){
  probs_30days[i] &lt;- mean(pnorm(X%*%sim_betas[i,]))
}
*quantile(probs_30days,probs = c(.025,.975))
```

```
##      2.5%     97.5% 
## 0.6205824 0.6694931
```

---


```r
bind_rows(
  tibble(pred = probs_30days, cond = "30 Days"),
  tibble(pred = probs_sameday, cond = "Same Day")
) %&gt;% 
  ggplot(aes(pred,fill=cond)) +
  geom_density(alpha=.6,color="white")
```

&lt;img src="week-12_ppol561_files/figure-html/unnamed-chunk-48-1.png" style="display: block; margin: auto;" /&gt;

---


```r
difference = probs_30days-probs_sameday
```

Is the difference between the two predictions statistically signficant?


```r
bounds = quantile(difference,probs = c(.025,.975))
tibble(
  lower = bounds[1],
  prob = mean(difference),
  higher = bounds[2],
)
```

```
## # A tibble: 1 x 3
##    lower    prob   higher
##    &lt;dbl&gt;   &lt;dbl&gt;    &lt;dbl&gt;
## 1 -0.120 -0.0648 -0.00550
```

--
&lt;br&gt;&lt;br&gt;

**Just barely!**

---

&lt;img src="week-12_ppol561_files/figure-html/unnamed-chunk-51-1.png" style="display: block; margin: auto;" /&gt;

---

### `obsval`

That's pretty involved! Is there a package? 

--

Of course...


```r
# Install package from Github (makes calculating sim values easy)
devtools::install_github("chrismeserole/obsval")
require(obsval)
```

--


```r
mod4 &lt;- obsval::obsval(vote~close+male+age,
               data=elect,
               reg.model = "probit",
               effect.var = "close",
               effect.vals = c(0,10,15,30,60,90))
apply(mod4$preds,2,mean)
```

```
##     var_0    var_10    var_15    var_30    var_60    var_90 
## 0.7101687 0.6891826 0.6783802 0.6448847 0.5743454 0.5021520
```

---


```r
as_tibble(mod4$preds) %&gt;% 
  gather(var,val) %&gt;% 
  ggplot(aes(var,val)) +
  geom_boxplot(size=1) +
  labs(y="Probability of Voting",
       x = "Close of Voter Registration") +
  theme(axis.text=element_text(size=16),
        axis.title=element_text(size=16))
```

&lt;img src="week-12_ppol561_files/figure-html/unnamed-chunk-54-1.png" style="display: block; margin: auto;" /&gt;

---



```r
elect %&gt;% 
  ggplot(aes(factor(close))) +
  geom_bar() +
  labs(x = "Close of Voter Registration",y="Count")
```

&lt;img src="week-12_ppol561_files/figure-html/unnamed-chunk-55-1.png" style="display: block; margin: auto;" /&gt;

---

## Note on the `obsval` output


```r
# stored as a list
class(mod4)
```

```
## [1] "list"
```


```r
# many different features of the prediction
names(mod4)
```

```
##  [1] "model"           "sim.coefs"       "preds"           "means"          
##  [5] "low.ci"          "high.ci"         "control.preds"   "control.mean"   
##  [9] "control.low.ci"  "control.high.ci" "effect.preds"    "effect.mean"    
## [13] "effect.low.ci"   "effect.high.ci"  "effect_sum"      "effect.var"     
## [17] "reg.model"
```


```r
# The N simulated probabilites for each manipulation
dim(mod4$preds)
```

```
## [1] 1000    6
```

---

## Note on the `obsval` output


```r
# Holds all the relevant summary values
tibble(cond = colnames(mod4$means),
       est = as.numeric(mod4$means),
       low = as.numeric(mod4$low.ci),
       high = as.numeric(mod4$high.ci)) 
```

```
## # A tibble: 6 x 4
##   cond     est   low  high
##   &lt;chr&gt;  &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;
## 1 var_0  0.710 0.662 0.755
## 2 var_10 0.689 0.658 0.720
## 3 var_15 0.678 0.654 0.703
## 4 var_30 0.645 0.622 0.669
## 5 var_60 0.574 0.495 0.656
## 6 var_90 0.502 0.364 0.650
```
    </textarea>
<style data-target="print-only">@media screen {.remark-slide-container{display:block;}.remark-slide-scaler{box-shadow:none;}}</style>
<script src="https://remarkjs.com/downloads/remark-latest.min.js"></script>
<script src="macros.js"></script>
<script>var slideshow = remark.create({
"highlightStyle": "github",
"countIncrementalSlides": false,
"highlightLines": true
});
if (window.HTMLWidgets) slideshow.on('afterShowSlide', function (slide) {
  window.dispatchEvent(new Event('resize'));
});
(function(d) {
  var s = d.createElement("style"), r = d.querySelector(".remark-slide-scaler");
  if (!r) return;
  s.type = "text/css"; s.innerHTML = "@page {size: " + r.style.width + " " + r.style.height +"; }";
  d.head.appendChild(s);
})(document);

(function(d) {
  var el = d.getElementsByClassName("remark-slides-area");
  if (!el) return;
  var slide, slides = slideshow.getSlides(), els = el[0].children;
  for (var i = 1; i < slides.length; i++) {
    slide = slides[i];
    if (slide.properties.continued === "true" || slide.properties.count === "false") {
      els[i - 1].className += ' has-continuation';
    }
  }
  var s = d.createElement("style");
  s.type = "text/css"; s.innerHTML = "@media print { .has-continuation { display: none; } }";
  d.head.appendChild(s);
})(document);
// delete the temporary CSS (for displaying all slides initially) when the user
// starts to view slides
(function() {
  var deleted = false;
  slideshow.on('beforeShowSlide', function(slide) {
    if (deleted) return;
    var sheets = document.styleSheets, node;
    for (var i = 0; i < sheets.length; i++) {
      node = sheets[i].ownerNode;
      if (node.dataset["target"] !== "print-only") continue;
      node.parentNode.removeChild(node);
    }
    deleted = true;
  });
})();
// adds .remark-code-has-line-highlighted class to <pre> parent elements
// of code chunks containing highlighted lines with class .remark-code-line-highlighted
(function(d) {
  const hlines = d.querySelectorAll('.remark-code-line-highlighted');
  const preParents = [];
  const findPreParent = function(line, p = 0) {
    if (p > 1) return null; // traverse up no further than grandparent
    const el = line.parentElement;
    return el.tagName === "PRE" ? el : findPreParent(el, ++p);
  };

  for (let line of hlines) {
    let pre = findPreParent(line);
    if (pre && !preParents.includes(pre)) preParents.push(pre);
  }
  preParents.forEach(p => p.classList.add("remark-code-has-line-highlighted"));
})(document);</script>

<script>
(function() {
  var links = document.getElementsByTagName('a');
  for (var i = 0; i < links.length; i++) {
    if (/^(https?:)?\/\//.test(links[i].getAttribute('href'))) {
      links[i].target = '_blank';
    }
  }
})();
</script>

<script>
slideshow._releaseMath = function(el) {
  var i, text, code, codes = el.getElementsByTagName('code');
  for (i = 0; i < codes.length;) {
    code = codes[i];
    if (code.parentNode.tagName !== 'PRE' && code.childElementCount === 0) {
      text = code.textContent;
      if (/^\\\((.|\s)+\\\)$/.test(text) || /^\\\[(.|\s)+\\\]$/.test(text) ||
          /^\$\$(.|\s)+\$\$$/.test(text) ||
          /^\\begin\{([^}]+)\}(.|\s)+\\end\{[^}]+\}$/.test(text)) {
        code.outerHTML = code.innerHTML;  // remove <code></code>
        continue;
      }
    }
    i++;
  }
};
slideshow._releaseMath(document);
</script>
<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
(function () {
  var script = document.createElement('script');
  script.type = 'text/javascript';
  script.src  = 'https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-MML-AM_CHTML';
  if (location.protocol !== 'file:' && /^https?:/.test(script.src))
    script.src  = script.src.replace(/^https?:/, '');
  document.getElementsByTagName('head')[0].appendChild(script);
})();
</script>
  </body>
</html>
